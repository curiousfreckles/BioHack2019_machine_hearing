{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ART1 demo\n",
    "\n",
    "Adaptive Resonance Theory Neural Networks\n",
    "by Aman Ahuja | github.com/amanahuja | twitter: @amanqa\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "Reminders: \n",
    "\n",
    " * ART1 accepts binary inputs only. \n",
    " *  \n",
    "\n",
    "In this example:\n",
    " * We'll use 10x10 ASCII blocks to demonstrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Load data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\"   O \",\n",
    "        \"  O O\",\n",
    "        \"    O\",\n",
    "        \"  O O\",\n",
    "        \"    O\",\n",
    "        \"  O O\",\n",
    "        \"    O\",\n",
    "        \" OO O\",\n",
    "        \" OO  \",\n",
    "        \" OO O\",\n",
    "        \" OO  \",\n",
    "        \"OOO  \",\n",
    "        \"OO   \",\n",
    "        \"O    \",\n",
    "        \"OO   \",\n",
    "        \"OOO  \",\n",
    "        \"OOOO \",\n",
    "        \"OOOOO\",\n",
    "        \"O    \",\n",
    "        \" O   \",\n",
    "        \"  O  \",\n",
    "        \"   O \",\n",
    "        \"    O\",\n",
    "        \"  O O\",\n",
    "        \" OO O\",\n",
    "        \" OO  \",\n",
    "        \"OOO  \",\n",
    "        \"OO   \",\n",
    "        \"OOOO \",\n",
    "        \"OOOOO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simplied ART1\n",
    "\n",
    "class ART1:\n",
    "    \"\"\"\n",
    "    ART class\n",
    "    modified Aman Ahuja\n",
    "    \n",
    "    Usage example:\n",
    "    --------------\n",
    "    # Create a ART network with input of size 5 and 20 internal units\n",
    "    >>> network = ART(5,10,0.5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=5, m=10, rho=.5):\n",
    "        '''\n",
    "        Create network with specified shape\n",
    "        \n",
    "        For Input array I of size n, we need n input nodes in F1. \n",
    "        \n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n : int\n",
    "            feature dimension of input; number of nodes in F1\n",
    "        m : int\n",
    "            Number of neurons in F2 competition layer\n",
    "            max number of categories\n",
    "            compare to n_class\n",
    "        rho : float\n",
    "            Vigilance parameter\n",
    "            larger rho: less inclusive prototypes\n",
    "            smaller rho: more generalization\n",
    "        \n",
    "        internal paramters\n",
    "        ---------- \n",
    "        F1: array of size (n)\n",
    "            array of F1 neurons\n",
    "        F2: array of size (m)\n",
    "            array of F2 neurons\n",
    "        Wf: array of shape (m x n)\n",
    "            Feed-Forward weights\n",
    "            These are Tk\n",
    "        Wb: array of shape (n x m)\n",
    "            Feed-back weights\n",
    "        n_cats : int\n",
    "            Number of F2 neurons that are active\n",
    "            (at any given time, number of category templates)\n",
    "        \n",
    "        '''\n",
    "        # Comparison layer\n",
    "        self.F1 = np.ones(n)\n",
    "        \n",
    "        # Recognition layer\n",
    "        self.F2 = np.ones(m)\n",
    "        \n",
    "        # Feed-forward weights\n",
    "        self.Wf = np.random.random((m,n))\n",
    "        \n",
    "        # Feed-back weights\n",
    "        self.Wb = np.random.random((n,m))\n",
    "        \n",
    "        # Vigilance parameter\n",
    "        self.rho = rho\n",
    "        \n",
    "        # Number of active units in F2\n",
    "        self.n_cats = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset whole network to start conditions\n",
    "        \"\"\"\n",
    "        self.F1 = np.ones(n)\n",
    "        self.F2 = np.ones(m)\n",
    "        self.Wf = np.random.random((m,n))\n",
    "        self.Wb = np.random.random((n,m))\n",
    "        self.n_cats = 0 \n",
    "        \n",
    "    def learn(self, X):\n",
    "        \"\"\"Learn X\n",
    "        use i as index over inputs or F1\n",
    "        use k as index over categories or F2\n",
    "        \"\"\" \n",
    "\n",
    "        # Compute F2 output using feed forward weights\n",
    "        self.F2[...] = np.dot(self.Wf, X)\n",
    "        \n",
    "        # collect and sort the output of each active node (C)\n",
    "        C = np.argsort(self.F2[:self.n_cats].ravel())[::-1]\n",
    "\n",
    "        for k in C:\n",
    "            # compute nearest memory\n",
    "            d = (self.Wb[:,k]*X).sum()/X.sum()\n",
    "\n",
    "            # Check if d is above the vigilance level\n",
    "            if d >= self.rho:\n",
    "                ww = self._learn_data(k, X)\n",
    "                return ww\n",
    "            else: \n",
    "                pass\n",
    "\n",
    "        # No match found within vigilance level\n",
    "        # If there's room, increase the number of active units\n",
    "        # and make the newly active unit to learn data\n",
    "        if self.n_cats < self.F2.size:\n",
    "            k = self.n_cats  # index of last category\n",
    "            ww = self._learn_data(k, X)\n",
    "            self.n_cats += 1\n",
    "            return ww\n",
    "        else: \n",
    "            return None,None\n",
    "\n",
    "    def _learn_data(self, node, dat):\n",
    "        \"\"\"\n",
    "        node : i : F2 node\n",
    "        dat  : X : input data\n",
    "        \"\"\" \n",
    "        self._validate_data(dat)\n",
    "        \n",
    "        # Learn data\n",
    "        self.Wb[:,node] *= dat\n",
    "        self.Wf[node,:] = self.Wb[:,node]/(0.5+self.Wb[:,node].sum())\n",
    "        return self.Wb[:,node], node\n",
    "    \n",
    "    def predict(self, X):\n",
    "        C = np.dot(self.Wf[:self.n_cats], X)\n",
    "\n",
    "        #return active F2 node, unless none are active\n",
    "        if np.all(C == 0):\n",
    "            return None\n",
    "\n",
    "        return np.argmax(C)\n",
    "\n",
    "    def _validate_data(self, dat):\n",
    "        \"\"\"\n",
    "        dat is a single input record\n",
    "        Checks: data must be 1s and 0s\n",
    "        \"\"\"\n",
    "        pass_checks = True\n",
    "        \n",
    "        # Dimensions must match\n",
    "        if dat.shape[0] != len(self.F1):\n",
    "            pass_checks = False\n",
    "            msg = \"Input dimensins mismatch.\"\n",
    "        \n",
    "        # Data must be 1s or 0s\n",
    "        if not np.all((dat == 1) | (dat == 0)):\n",
    "            pass_checks = False\n",
    "            msg = \"Input must be binary.\"\n",
    "        \n",
    "        if pass_checks:\n",
    "            return True\n",
    "        else: \n",
    "            raise Exception(\"Data does not validate: {}\".format(msg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def preprocess_data(data): \n",
    "    \"\"\"\n",
    "    Convert to numpy array\n",
    "    Convert to 1s and 0s\n",
    "    \n",
    "    \"\"\"\n",
    "    # Look at first row\n",
    "    if data[0]: \n",
    "        irow = data[0]\n",
    "\n",
    "        # get size\n",
    "        idat_size = len(irow)\n",
    "\n",
    "        # get unique characters\n",
    "        chars = False\n",
    "        while not chars: \n",
    "            chars = get_unique_chars(irow, reverse=True)\n",
    "        char1, char2 = chars\n",
    "\n",
    "    outdata = []\n",
    "    idat = np.zeros(idat_size, dtype=bool)\n",
    "\n",
    "    for irow in data:\n",
    "        idat = [x==char1 for x in irow]\n",
    "        outdata.append(idat)\n",
    "    \n",
    "    return np.array(outdata).astype(int)\n",
    "\n",
    "def get_unique_chars(irow, reverse=False):\n",
    "    \"\"\"\n",
    "    Get unique characters in data\n",
    "    Helper function\n",
    "    ---- \n",
    "    reverse:   bool\n",
    "        Reverses order of the two chars returned\n",
    "    \"\"\"\n",
    "    chars = Counter(irow)\n",
    "    if len(chars) > 2: \n",
    "        raise Exception(\"Data is not binary\")\n",
    "    elif len(chars) < 2: \n",
    "        # first row doesn't contain both chars\n",
    "        return False, False\n",
    "\n",
    "    # Reorder here?\n",
    "    if reverse: \n",
    "        char2, char1 = chars.keys()\n",
    "    else: \n",
    "        char1, char2 = chars.keys()\n",
    "    \n",
    "    return char1, char2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = ART1(n=5, m=7, rho=0.5)\n",
    "\n",
    "# preprocess data\n",
    "data_cleaned = cat.astype(int).values\n",
    "data_cleaned\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predictions\n",
    "\n",
    "Let's see the clusters created through training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.n_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.defaultdict' object has no attribute 'iteritems'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1303443857e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moutput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-----'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.defaultdict' object has no attribute 'iteritems'"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "output_dict = defaultdict(list)\n",
    "\n",
    "for row, row_cleaned in zip (data, data_cleaned): \n",
    "    pred = network.predict(row_cleaned)\n",
    "    output_dict[pred].append(row)\n",
    "\n",
    "for k,v in output_dict.iteritems():\n",
    "    print(k)\n",
    "    print('-----')\n",
    "    for row in v: \n",
    "        print(row)\n",
    "    print()\n",
    "#   \\  print \"'{}':{}\".format(\n",
    "#         row, \n",
    "#         network.predict(row_cleaned))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the weights as patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern #0\n",
      "[0 0 0 1 0]\n",
      "\n",
      "Pattern #1\n",
      "[0 0 1 0 1]\n",
      "\n",
      "Pattern #2\n",
      "[0 0 0 0 1]\n",
      "\n",
      "Pattern #3\n",
      "[0 0 1 0 0]\n",
      "\n",
      "Pattern #4\n",
      "[0 0 0 0 1]\n",
      "\n",
      "Pattern #5\n",
      "[0 0 1 0 1]\n",
      "\n",
      "Pattern #6\n",
      "[0 0 0 0 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_units = network.Wf[:network.n_cats]\n",
    "for idx, CU_weights in enumerate(cluster_units):\n",
    "    pattern = CU_weights\n",
    "    pattern = pattern.astype(bool)\n",
    "    \n",
    "    print(\"Pattern #{}\".format(idx))\n",
    "    print(pattern.astype(int))\n",
    "    print()\n",
    "    #preprocess_data(pattern)\n",
    "    #print np.round(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "mat250 = scipy.io.loadmat('/Users/xtinaushakova/Play/ushi/data/spectra/ANprob41HSR_44kHz0250Hz.mat')\n",
    "mat500 = scipy.io.loadmat('/Users/xtinaushakova/Play/ushi/data/spectra/ANprob41HSR_44kHz0500Hz.mat')\n",
    "mat1000 = scipy.io.loadmat('/Users/xtinaushakova/Play/ushi/data/spectra/ANprob41HSR_44kHz1000Hz.mat')\n",
    "mat2000 = scipy.io.loadmat('/Users/xtinaushakova/Play/ushi/data/spectra/ANprob41HSR_44kHz2000Hz.mat')\n",
    "mat4000 = scipy.io.loadmat('/Users/xtinaushakova/Play/ushi/data/spectra/ANprob41HSR_44kHz4000Hz.mat')\n",
    "mat7000 = scipy.io.loadmat('/Users/xtinaushakova/Play/ushi/data/spectra/ANprob41HSR_44kHz7000Hz.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe\n",
    "mat250 = pd.DataFrame(mat250['ANprobRateOutput'])\n",
    "mat500 = pd.DataFrame(mat500['ANprobRateOutput'])\n",
    "mat1000 = pd.DataFrame(mat1000['ANprobRateOutput'])\n",
    "mat2000 = pd.DataFrame(mat2000['ANprobRateOutput'])\n",
    "mat4000 = pd.DataFrame(mat4000['ANprobRateOutput'])\n",
    "mat7000 = pd.DataFrame(mat7000['ANprobRateOutput'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize and round to get binary data\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "mat250_scaled = round(pd.DataFrame(min_max_scaler.fit_transform(mat250)).transpose())\n",
    "mat500_scaled = round(pd.DataFrame(min_max_scaler.fit_transform(mat500)).transpose())\n",
    "mat1000_scaled = round(pd.DataFrame(min_max_scaler.fit_transform(mat1000)).transpose())\n",
    "mat2000_scaled = round(pd.DataFrame(min_max_scaler.fit_transform(mat2000)).transpose())\n",
    "mat4000_scaled = round(pd.DataFrame(min_max_scaler.fit_transform(mat4000)).transpose())\n",
    "mat7000_scaled = round(pd.DataFrame(min_max_scaler.fit_transform(mat7000)).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate into one long shabang\n",
    "cat = pd.concat([mat250_scaled, mat500_scaled, mat1000_scaled, mat2000_scaled, mat4000_scaled, mat7000_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xtinaushakova/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:91: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n categories:  7\n",
      "Pattern #0\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0]\n",
      "\n",
      "Pattern #1\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 0]\n",
      "\n",
      "Pattern #2\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 0]\n",
      "\n",
      "Pattern #3\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 0]\n",
      "\n",
      "Pattern #4\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 0 0 0 0]\n",
      "\n",
      "Pattern #5\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0]\n",
      "\n",
      "Pattern #6\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = ART1(n=41, m=7, rho=0.5)\n",
    "\n",
    "# preprocess data\n",
    "\n",
    "\n",
    "# learn data array, row by row\n",
    "for row in data_cleaned:\n",
    "    network.learn(row)\n",
    "\n",
    "print(\"n categories: \", network.n_cats)\n",
    "#print tt\n",
    "\n",
    "cluster_units = network.Wf[:network.n_cats]\n",
    "for idx, CU_weights in enumerate(cluster_units):\n",
    "    pattern = CU_weights\n",
    "    pattern = pattern.astype(bool)\n",
    "    \n",
    "    print(\"Pattern #{}\".format(idx))\n",
    "    print(pattern.astype(int))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
